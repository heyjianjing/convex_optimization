{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Decomposable functions"
      ],
      "metadata": {
        "id": "u6acydXJCNEC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall the concept of `gradient descent` is to, at iteration $k$, minimize the quadratic approximation of $f$ at $x^k$\n",
        "\n",
        "$$x^{k+1}=\\arg \\min_xf(x^{k})+\\nabla f(x^{k})^T(x-x^{k})+\\frac{1}{2t^k}\\|x-x^{k}\\|_2^2$$\n",
        "\n",
        "We take derivative and set it to zero\n",
        "\n",
        "$$0+\\nabla f(x^{k})+\\frac{1}{t^k}(x-x^{k})=0$$\n",
        "\n",
        "and we get the gradient descent equation\n",
        "\n",
        "$$x^{k+1}=x^{k}-t^k \\nabla f(x^{k})$$"
      ],
      "metadata": {
        "id": "u4GjcTQXAz2p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main motivation of proximal gradient method is that, when $f$ is not differentiable, rather than replacing the whole thing using subgradient, which we have seen is slow, we would like to keep some portion of this quadratic approximation\n",
        "\n",
        "In particular, we look at function\n",
        "\n",
        "$$f(x)=g(x)+h(x)$$\n",
        "\n",
        "where $g$ is differentiable while $h$ is not\n",
        "\n",
        "and we apply the quadratic approximation to $g$\n",
        "\n",
        "$$\\begin{align*}\n",
        "x^{k+1} &  = \\arg \\min_z g(x^k)+\\nabla g(x^k)^T(z-x^k)+\\frac{1}{2t}\\|z-x^k\\|_2^2+h(z)\\\\\n",
        "& g(x^k), \\nabla g(x^k) \\text{ as individual term has no effect on minimization over z}\\\\\n",
        " & = \\arg \\min_z \\frac{1}{2} t\\|\\nabla g(x^k)\\|_2^2+\\nabla g(x^k)^T(z-x^k)+\\frac{1}{2t}\\|z-x^k\\|_2^2+h(z)\\\\\n",
        "&=\\arg\\min_z\\frac{1}{2t}\\|z-\\left(x^k-t\\nabla g(x^k)\\right)\\|_2^2+h(z)\n",
        "\\end{align*}$$\n",
        "\n",
        "That is, we try to find a $z$ such that\n",
        "\n",
        "* it stays close to gradient update of $g$ by minimizing $\\|z-\\left(x^k-t\\nabla g(x^k)\\right)\\|_2^2$\n",
        "* it makes $h$ small by minimizing $h(z)$"
      ],
      "metadata": {
        "id": "pwiTSDI4GFhl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Proximal operator"
      ],
      "metadata": {
        "id": "swN73F-nKe4o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "More formally, we define proximal operator as\n",
        "\n",
        "$$\\text{prox}_{h, t}(x) = \\arg \\min_z \\frac{1}{2t}\\|z-x\\|_2^2+h(z) $$\n",
        "\n",
        "As an example, if $h(z)$ is the indicator function, then\n",
        "\n",
        "$$\\begin{align*}\n",
        "\\text{prox}_{I, t}(x) &= \\arg \\min_z \\frac{1}{2t}\\|z-x\\|_2^2+I_S(z) \\\\\n",
        "&=\\arg \\min_z \\frac{1}{2t}\\|z-x\\|_2^2, \\text{s.t. }z\\in S\n",
        "\\end{align*}$$\n"
      ],
      "metadata": {
        "id": "zz5j_dQQKhRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use proximal operator, we can write proximal gradient step for $f(x)=g(x)+h(x)$\n",
        "\n",
        "$$x^{k+1} = \\text{prox}_{h, t}(x^k-t\\nabla g(x^k))$$\n",
        "\n",
        "or we can write it more like standard gradient step by defining\n",
        "\n",
        "$$G_t(x)=\\left(x-\\text{prox}_{h, t}\\left(x-t\\nabla g(x)\\right)\\right)/t$$\n",
        "\n",
        "which is often called generalized gradient of $f$\n",
        "\n",
        "With this, we can write the proximal gradient step as\n",
        "\n",
        "$$x^{k+1}=x^k-tG_{t}(x^k)$$"
      ],
      "metadata": {
        "id": "niC5IIAXLSr7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The key point for proximal method is that\n",
        "* $\\text{prox}_{h, t}(\\cdot)$ often has `closed-form` expression for many commonly used $h$ or can be computed very efficiently, and it only depends on $h$\n",
        "* The $g$ part can be complicated, but we only need its gradient"
      ],
      "metadata": {
        "id": "4l2dCnIkIy7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Backtracking line search"
      ],
      "metadata": {
        "id": "UbwwGgtDSLgd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Backtracking line search for proximal method works similarly as with gradient descent\n",
        "\n",
        "We choose $\\beta\\in (0,1)$, at each iteration, we start with some $t$ (e.g., from previous iteration) and let\n",
        "\n",
        "$$z=\\text{prox}_{h, t}(x^k-t\\nabla g(x^k))$$\n",
        "\n",
        "while\n",
        "\n",
        "$$g(z)>g(x^k)+\\nabla g(x^k)^T (z-x^k)+\\frac{1}{2t}\\|z-x^k\\|_2^2$$\n",
        "\n",
        "we do $t\\leftarrow \\beta t$\n",
        "\n",
        "Else, we perform proximal gradient update with $x^{k+1}=z$"
      ],
      "metadata": {
        "id": "kRZJHbLRSNev"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example: LASSO"
      ],
      "metadata": {
        "id": "Y7mXB4LcLbmU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we use proximal gradient method for LASSO example\n",
        "\n",
        "$$\\min_x \\frac{1}{2}\\|y-Ax\\|_2^2+\\lambda \\|x\\|_1, \\lambda \\geq 0$$\n",
        "\n",
        "where $g(x)=\\frac{1}{2}\\|y-Ax\\|_2^2$ and $h(x)=\\lambda\\|x\\|_1$"
      ],
      "metadata": {
        "id": "78oV-TKhLh9R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get proximal gradient step\n",
        "\n",
        "$$x^{k+1} = \\text{prox}_{h, t}(x^k-t\\nabla g(x^k))$$\n",
        "\n",
        "we first compute gradient of $g$\n",
        "\n",
        "$$\\nabla g(x)=-A^T(y-Ax)$$"
      ],
      "metadata": {
        "id": "UrDkiniXMwqs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we compute proximal mapping for $h$\n",
        "\n",
        "$$\\text{prox}_{\\lambda\\|\\cdot\\|_1, t}(x)=\\arg \\min_z \\frac{1}{2t}\\|x-z\\|_2^2+\\lambda \\|z\\|_1$$\n",
        "\n",
        "This is essentially soft thresholding and we know from example on subgradient optimality condition\n",
        "\n",
        "We define $S_{\\lambda, t}(x)$ as soft thresholding operator and we have\n",
        "\n",
        "$$z_i = [S_{\\lambda, t}(x)]_i = \\left\\{\\begin{array}{ll} x_i-t\\lambda & x_i>t\\lambda \\\\ 0 & x_i \\in [-t\\lambda, t\\lambda] \\\\ x_i+t\\lambda & x_i<-t\\lambda\\end{array}\\right.$$"
      ],
      "metadata": {
        "id": "eOG5I2jmMtqw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combine these two, we have the proximal gradient step for LASSO\n",
        "\n",
        "$$x^{k+1}=S_{\\lambda, t}\\left(x^k+tA^T(y-Ax^k)\\right)$$\n",
        "\n",
        "This algorithm is commonly known as Iterative Shrinkage Thresholding Algorithm (ISTA)"
      ],
      "metadata": {
        "id": "wOe8PDo8NBc_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wCZw3atoMDrj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}