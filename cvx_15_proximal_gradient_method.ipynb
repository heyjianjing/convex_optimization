{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Decomposable functions"
      ],
      "metadata": {
        "id": "u6acydXJCNEC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall the concept of `gradient descent` is to, at iteration $k$, minimize the quadratic approximation of $f$ at $x^k$\n",
        "\n",
        "$$x^{k+1}=\\arg \\min_xf(x^{k})+\\nabla f(x^{k})^T(x-x^{k})+\\frac{1}{2t^k}\\|x-x^{k}\\|_2^2$$\n",
        "\n",
        "We take derivative and set it to zero\n",
        "\n",
        "$$0+\\nabla f(x^{k})+\\frac{1}{t^k}(x-x^{k})=0$$\n",
        "\n",
        "and we get the gradient descent equation\n",
        "\n",
        "$$x^{k+1}=x^{k}-t^k \\nabla f(x^{k})$$"
      ],
      "metadata": {
        "id": "u4GjcTQXAz2p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main motivation of proximal gradient method is that, when $f$ is not differentiable, rather than replacing the whole thing using subgradient, which we have seen is slow, we would like to keep some portion of this quadratic approximation\n",
        "\n",
        "In particular, we look at function\n",
        "\n",
        "$$f(x)=g(x)+h(x)$$\n",
        "\n",
        "where $g$ is differentiable while $h$ is not\n",
        "\n",
        "and we apply the quadratic approximation to $g$\n",
        "\n",
        "$$\\begin{align*}\n",
        "x^{k+1} &  = \\arg \\min_z g(x^k)+\\nabla g(x^k)^T(z-x^k)+\\frac{1}{2t}\\|z-x^k\\|_2^2+h(z)\\\\\n",
        "& g(x^k), \\nabla g(x^k) \\text{ as individual term has no effect on minimization over z}\\\\\n",
        " & = \\arg \\min_z \\frac{1}{2} t\\|\\nabla g(x^k)\\|_2^2+\\nabla g(x^k)^T(z-x^k)+\\frac{1}{2t}\\|z-x^k\\|_2^2+h(z)\\\\\n",
        "&=\\arg\\min_z\\frac{1}{2t}\\|z-\\left(x^k-t\\nabla g(x^k)\\right)\\|_2^2+h(z)\n",
        "\\end{align*}$$\n",
        "\n",
        "That is, we try to find a $z$ such that\n",
        "\n",
        "* it stays close to gradient update of $g$ by minimizing $\\|z-\\left(x^k-t\\nabla g(x^k)\\right)\\|_2^2$\n",
        "* it makes $h$ small by minimizing $h(z)$"
      ],
      "metadata": {
        "id": "pwiTSDI4GFhl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Proximal operator"
      ],
      "metadata": {
        "id": "swN73F-nKe4o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "More formally, we define proximal operator as\n",
        "\n",
        "$$\\text{prox}_{th}(x) = \\arg \\min_z \\frac{1}{2t}\\|z-x\\|_2^2+h(z) $$\n",
        "\n",
        "and the proximal gradient step is\n",
        "\n",
        "$$x^{k+1} = \\text{prox}_{th}(x^k-t\\nabla g(x^k))$$"
      ],
      "metadata": {
        "id": "zz5j_dQQKhRP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wCZw3atoMDrj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}