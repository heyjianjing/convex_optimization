{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Subgradient and nondifferentiable function"
      ],
      "metadata": {
        "id": "mzbsbdGs2Xpg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A nondifferentiable function $f(x)$ is convex if and only if\n",
        "\n",
        "for any $x_0\\in \\text{dom}f$, there exists a vector $g$ such that\n",
        "\n",
        "$$f(x)\\geq f(x_0) + g(x_0)^T(x-x_0)$$\n",
        "\n",
        "i.e., `global underestimator` of $f$\n",
        "\n",
        "$g$ is refered to as `subgradient`\n",
        "\n",
        "Consider function $f(x)=|x|$, at $x=0$, there are infinite number of subgradients $g$, and the `set` of subgradients of $f$ at a point $x$ is called `subdifferential` of $f$ at $x$, denoted as $\\partial f(x)$"
      ],
      "metadata": {
        "id": "AaoBp6RJ2aWt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nondifferentiable $f$ is convex if and only if its subgradient is `monotone`\n",
        "\n",
        "$$\\langle g(x_1) - g(x_2) ,x_1-x_2 \\rangle \\geq 0, \\forall g(x_1)\\in \\partial f(x_1), g(x_2)\\in \\partial f(x_2)$$"
      ],
      "metadata": {
        "id": "sDzFA5FS7MR6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Some basic rules"
      ],
      "metadata": {
        "id": "YNiBo0kmLDW8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$f$ is convex\n",
        "\n",
        "* $\\partial f(x)=\\{\\nabla f(x)\\}$ if $f$ is differentiable at $x$\n",
        "* **scaling**: $\\partial(\\alpha f)=\\alpha \\partial f, \\alpha>0$\n",
        "* **addition**: $\\partial(f_1+f_2)=\\partial f_1 +\\partial f_2$\n",
        "* **affine transformation**: if $g(x)=f(Ax+b)$, then $\\partial g(x)=A^T \\partial f(Ax+b)$\n",
        "* **finite pointwise maximum**: if $f=\\max_i f_i$, then subdifferential is the `convex hull` of union of subdifferentials of `active` functions at $x$\n",
        "$$\\partial f(x)= \\text{conv} \\bigcup\\{\\partial f_i(x)|f_i(x)=f(x)\\}$$\n",
        "* **pointwise supremum**: if $f=\\sup_{\\alpha \\in A} f_{\\alpha}$, then, roughly speaking, $\\partial f(x)$ is closure of convex hull of union of subdifferentials of active functions\n",
        "$$\\text{cl conv}\\bigcup \\{\\partial f_{\\beta}(x)|f_{\\beta}(x)=f(x)\\}\\subseteq \\partial f(x)$$\n",
        "* **weak rule** for pointwise supremum: find one $g\\in \\partial f(x)$\n",
        "    * find any $\\beta$ for which $f_{\\beta}(x)=f(x)$ (assume supremum is achieved)\n",
        "    * choose any $g\\in \\partial f_{\\beta}(x)$"
      ],
      "metadata": {
        "id": "dz8sEPlRLE8n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example"
      ],
      "metadata": {
        "id": "3g2ZAiEVRrPB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Maximum eigenvalue"
      ],
      "metadata": {
        "id": "UWznMg3xGxDQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$f(x)=\\lambda_{\\max}(A(x))=\\sup_{\\|y\\|_2=1} y^TA(x)y$$\n",
        "\n",
        "where $A(x)=A_0+x_1A_1+\\cdots + x_nA_n$\n",
        "\n",
        "* $f$ is pointwise supremum of $g_y(x)=y^TA(x)y\\,$ over $\\|y\\|_2=1$\n",
        "* for any $x$, the supremum is achieved when $y$ satisfies $A(x)y=\\lambda_{max}(A(x))y, \\|y\\|_2=1$\n",
        "* $\\nabla g_y(x)=\\begin{bmatrix}y^TA_1y & \\cdots & y^TA_n(x)y\\end{bmatrix}^T$\n",
        "\n",
        "Therefore, to find `a` subgradient $\\in \\partial f$ at $x$, we can choose any unit eigenvector $y$ associated with $\\lambda_{\\max} (A(x))$ and\n",
        "\n",
        "$$\\begin{bmatrix}y^TA_1y & \\cdots & y^TA_n(x)y\\end{bmatrix}^T\\in \\partial f(x)$$"
      ],
      "metadata": {
        "id": "1yjEDtgVRuFD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Expectation"
      ],
      "metadata": {
        "id": "QQ5uVfaQJcnZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$f(x)=\\mathbb{E}_uf(x, u)$$\n",
        "\n",
        "We can use Monte Carlo method\n",
        "\n",
        "* Sample $u_{1, \\cdots, k}$\n",
        "* $f(x) \\approx (1/k)\\sum_{i=1}^kf(x, u_i)$\n",
        "* for each $i$, choose a $g(x, u_i)\\in \\partial_xf(x,  u_i)$\n",
        "* We obtain an approximate subgradient $g=(1/k)\\sum_{i=1}^kg(x, u_i)$"
      ],
      "metadata": {
        "id": "nyvEh_24JeFA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Minimization"
      ],
      "metadata": {
        "id": "ZpHNmxbLLBJw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define $g(y)$ as the optimal value of\n",
        "\n",
        "$$\\min f_0(x), \\,\\,\\ \\text{s.t. } f_i(x)\\leq y_i$$\n",
        "\n",
        "where $f_i$ are convex\n",
        "\n",
        "If `strong duality` holds with the dual\n",
        "\n",
        "$$\\max \\inf_x \\left(f_0(x)+\\sum_{i=1}^m\\lambda_i\\left(f_i(x)-y_i\\right)\\right), \\,\\, \\text{s.t. }\\lambda\\geq 0$$\n",
        "\n",
        "for which $\\lambda^*$ is its dual optimal\n",
        "\n",
        "then, for a $z$ such that $g(z)$ is finite, we have\n",
        "\n",
        "$$\\begin{align*}\n",
        "g(z)&\\geq\\inf_x \\left(f_0(x)+\\sum_{i=1}^m\\lambda^*_i\\left(f_i(x)-z_i\\right)\\right) \\\\\n",
        "&=\\inf_x \\left(f_0(x)+\\sum_{i=1}^m\\lambda^*_i\\left(f_i(x)-y_i\\right)\\right) - \\sum_{i=1}^m\\lambda^*_i(z_i-y_i)\\\\\n",
        "& \\text{Strong duality} \\\\\n",
        "&=g(y) - \\sum_{i=1}^m\\lambda^*_i(z_i-y_i)\n",
        "\\end{align*}$$\n",
        "\n",
        "That is, $-\\lambda^*$ is a subgradient of $g$ at $y$"
      ],
      "metadata": {
        "id": "GTT3RKmZLCYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Optimality condition"
      ],
      "metadata": {
        "id": "2UmF56XP1Dns"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Unconstrained optimzation"
      ],
      "metadata": {
        "id": "gj7k8O_fwAx3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If $f$ is convex and nondifferentiable, then $x^*$ minimizes $f(x)$ if and only if\n",
        "\n",
        "$$\\boxed{0 \\in \\partial f(x^*)}$$\n",
        "\n",
        "This follows directly from definition of subgradient\n",
        "\n",
        "$$f(y)\\geq f(x^*) =  f(x^*)+\\mathbf{0}^T(y-x^*), \\forall y$$"
      ],
      "metadata": {
        "id": "qaE1R77Y1Gsu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Example: piecewise linear minimization"
      ],
      "metadata": {
        "id": "paNeVHx-329a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$f(x)=\\max_i (a_i^Tx+b_i)$\n",
        "\n",
        "$x^*$ minimizes $f \\Longleftrightarrow 0\\in \\partial f(x^*) =\\text{conv}\\{a_i | i\\in I(x^*)\\}$\n",
        "\n",
        "where $I(x)=\\{i| a_i^Tx+b_i=f(x)\\}$\n",
        "\n",
        "By definition of convex hull, above is equivalent to\n",
        "\n",
        "$$\\exists \\lambda \\text{ s.t. } \\lambda\\geq 0, \\mathbf{1}^T\\lambda =1, \\sum_{i=1}^m\\lambda_i a_i=0$$\n",
        "\n",
        "Since the convex hull only applies to `active` $a_i$ (i.e., $a_i^Tx^*+b_i=f(x^*)$), therefore, for the `inactive` ones (i.e., $a_i^Tx^*+b_i<f(x^*)$), we have $\\lambda_i=0$"
      ],
      "metadata": {
        "id": "T4bda-vk35uZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that these are the KKT conditions for epigraph form\n",
        "\n",
        "$$\\min_{x, t} t, \\,\\, \\text{s.t. } a_i^Tx+b_i\\leq t$$"
      ],
      "metadata": {
        "id": "3CicXNThtFQA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Constrained optimization"
      ],
      "metadata": {
        "id": "6UVlKbyLwDDu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For\n",
        "\n",
        "$$\\min f_0(x), \\,\\, \\text{s.t. } f_i(x)\\leq 0$$\n",
        "\n",
        "If $f_i$ is convex and subdifferentiable, the problem is strictly feasible, then\n",
        "\n",
        "$x^*$ is primal optimal and $\\lambda^*$ is dual optimal if and only if\n",
        "\n",
        "$$\\begin{align*}\n",
        "& f_i(x^*)\\leq 0, \\,\\, \\lambda^*\\geq 0 \\\\\n",
        "& 0 \\in \\partial f_0(x^*)+\\sum_{i=1}^m\\lambda_i^*\\partial f_i(x^*)\\\\\n",
        "& \\lambda_i^* f_i(x^*)=0\n",
        "\\end{align*}$$\n",
        "\n",
        "This generalizes KKT conditions"
      ],
      "metadata": {
        "id": "LLZnkfTcwGdD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Directional derivative"
      ],
      "metadata": {
        "id": "zHAjJQmSHSoh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `directional derivative` of $f$ at $x$ in the direction of $\\delta x$ is defined as\n",
        "\n",
        "$$\\begin{align*}f'(x;\\delta x)&=\\lim_{a\\rightarrow 0} \\frac{f(x+a\\delta x)-f(x)}{a}\\\\\n",
        "&=\\lim_{t\\rightarrow \\infty}\\left(tf(x+\\frac{1}{t}\\delta x)-tf(x)\\right)\n",
        "\\end{align*}$$\n",
        "\n",
        "If $f$ is `differentiable`, then\n",
        "\n",
        "$$f(x+a\\delta x)-f(x)=f(x)+a\\nabla f(x)^T\\delta x + (a^2/2)\\delta x^THf(x)\\delta x + O(a^3)-f(x)$$\n",
        "\n",
        "Plug in and simplify, we have\n",
        "\n",
        "$$f'(x; \\delta x)=\\nabla f(x)^T \\delta x$$\n",
        "\n",
        "That is, the directional derivative is a linear function of $\\delta x$"
      ],
      "metadata": {
        "id": "7KsjGIdnHUpJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An equivalent definition for directional derivative of a `convex` $f$ is\n",
        "\n",
        "$$\\begin{align*}f'(x;\\delta x)&=\\inf_{a> 0} \\frac{f(x+a\\delta x)-f(x)}{a}\\\\\n",
        "&=\\inf_{t> 0} \\left(tf(x+\\frac{1}{t}\\delta x)-tf(x)\\right)\n",
        "\\end{align*}$$\n",
        "\n",
        "We can see this by defining $h(\\delta x)=f(x+\\delta x)-f(x)$, which is convex in $\\delta x$\n",
        "\n",
        "With $h(0)=0$, the perspective of $h(\\delta x)$\n",
        "\n",
        "$$th(\\delta x/t)=tf(x+\\frac{1}{t}\\delta x)-tf(x)$$\n",
        "\n",
        "is `nonincreasing` in $t>0$\n",
        "\n",
        "Therefore\n",
        "\n",
        "$$\\lim_{t\\rightarrow \\infty} th(\\delta x/t)=\\inf_{t>0} th(\\delta x/t)$$"
      ],
      "metadata": {
        "id": "uEpf9dkRJpSM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a result, $f'(x;\\delta x)$ defines a lower bound on $f$ in the direction of $\\delta x$\n",
        "\n",
        "$$f(x+a\\delta x)\\geq f(x) + a f'(x;\\delta x), \\forall a\\geq 0$$"
      ],
      "metadata": {
        "id": "K3JAUQ0AzGEy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DimhKwSjKe9e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}