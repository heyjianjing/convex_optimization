{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Subgradient and nondifferentiable function"
      ],
      "metadata": {
        "id": "mzbsbdGs2Xpg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A nondifferentiable function $f(x)$ is convex if and only if for any $x_0\\in \\text{dom}f$, there exists a vector $g$ such that\n",
        "\n",
        "$$f(x)\\geq f(x_0) + g(x_0)^T(x-x_0)$$\n",
        "\n",
        "i.e., `global underestimator` of $f$\n",
        "\n",
        "$g$ is refered to as `subgradient`\n",
        "\n",
        "Consider function $f(x)=|x|$, at $x=0$, there are infinite number of subgradients $g$, and the `set` of subgradients of $f$ at a point $x$ is called `subdifferential` of $f$ at $x$, denoted as $\\partial f(x)$"
      ],
      "metadata": {
        "id": "AaoBp6RJ2aWt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nondifferentiable $f$ is convex if and only if its subgradient is `monotone`\n",
        "\n",
        "$$\\langle g(x_1) - g(x_2) ,x_1-x_2 \\rangle \\geq 0, \\forall g(x_1)\\in \\partial f(x_1), g(x_2)\\in \\partial f(x_2)$$"
      ],
      "metadata": {
        "id": "sDzFA5FS7MR6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Some basic rules"
      ],
      "metadata": {
        "id": "YNiBo0kmLDW8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$f$ is convex\n",
        "\n",
        "* $\\partial f(x)=\\{\\nabla f(x)\\}$ if $f$ is differentiable at $x$\n",
        "* **scaling**: $\\partial(\\alpha f)=\\alpha \\partial f, \\alpha>0$\n",
        "* **addition**: $\\partial(f_1+f_2)=\\partial f_1 +\\partial f_2$\n",
        "* **affine transformation**: if $g(x)=f(Ax+b)$, then $\\partial g(x)=A^T \\partial f(Ax+b)$\n",
        "* **finite pointwise maximum**: if $f=\\max_i f_i$, then subdifferential is the `convex hull` of union of subdifferentials of `active` functions at $x$\n",
        "$$\\partial f(x)= \\text{conv} \\bigcup\\{\\partial f_i(x)|f_i(x)=f(x)\\}$$\n",
        "* **pointwise supremum**: if $f=\\sup_{\\alpha \\in A} f_{\\alpha}$, then, roughly speaking, $\\partial f(x)$ is closure of convex hull of union of subdifferentials of active functions\n",
        "$$\\text{cl conv}\\bigcup \\{\\partial f_{\\beta}(x)|f_{\\beta}(x)=f(x)\\}\\subseteq \\partial f(x)$$\n",
        "* **weak rule** for pointwise supremum: find one $g\\in \\partial f(x)$\n",
        "    * find any $\\beta$ for which $f_{\\beta}(x)=f(x)$ (assume supremum is achieved)\n",
        "    * choose any $g\\in \\partial f_{\\beta}(x)$"
      ],
      "metadata": {
        "id": "dz8sEPlRLE8n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example"
      ],
      "metadata": {
        "id": "3g2ZAiEVRrPB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Maximum eigenvalue"
      ],
      "metadata": {
        "id": "UWznMg3xGxDQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$f(x)=\\lambda_{\\max}(A(x))=\\sup_{\\|y\\|_2=1} y^TA(x)y$$\n",
        "\n",
        "where $A(x)=A_0+x_1A_1+\\cdots + x_nA_n$\n",
        "\n",
        "* $f$ is pointwise supremum of $g_y(x)=y^TA(x)y\\,$ over $\\|y\\|_2=1$\n",
        "* for any $x$, the supremum is achieved when $y$ satisfies $A(x)y=\\lambda_{max}(A(x))y, \\|y\\|_2=1$\n",
        "* $\\nabla g_y(x)=\\begin{bmatrix}y^TA_1y & \\cdots & y^TA_n(x)y\\end{bmatrix}^T$\n",
        "\n",
        "Therefore, to find `a` subgradient $\\in \\partial f$ at $x$, we can choose any unit eigenvector $y$ associated with $\\lambda_{\\max} (A(x))$ and\n",
        "\n",
        "$$\\begin{bmatrix}y^TA_1y & \\cdots & y^TA_n(x)y\\end{bmatrix}^T\\in \\partial f(x)$$"
      ],
      "metadata": {
        "id": "1yjEDtgVRuFD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Expectation"
      ],
      "metadata": {
        "id": "QQ5uVfaQJcnZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$f(x)=\\mathbb{E}_uf(x, u)$$\n",
        "\n",
        "We can use Monte Carlo method\n",
        "\n",
        "* Sample $u_{1, \\cdots, k}$\n",
        "* $f(x) \\approx (1/k)\\sum_{i=1}^kf(x, u_i)$\n",
        "* for each $i$, choose a $g(x, u_i)\\in \\partial_xf(x,  u_i)$\n",
        "* We obtain an approximate subgradient $g=(1/k)\\sum_{i=1}^kg(x, u_i)$"
      ],
      "metadata": {
        "id": "nyvEh_24JeFA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Minimization"
      ],
      "metadata": {
        "id": "ZpHNmxbLLBJw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define $g(y)$ as the optimal value of\n",
        "\n",
        "$$\\min f_0(x), \\,\\,\\ \\text{s.t. } f_i(x)\\leq y_i$$\n",
        "\n",
        "where $f_i$ are convex\n",
        "\n",
        "If `strong duality` holds with the dual\n",
        "\n",
        "$$\\max \\inf_x \\left(f_0(x)+\\sum_{i=1}^m\\lambda_i\\left(f_i(x)-y_i\\right)\\right), \\,\\, \\text{s.t. }\\lambda\\geq 0$$\n",
        "\n",
        "for which $\\lambda^*$ is its dual optimal\n",
        "\n",
        "Then, for a $z$ such that $g(z)$ is finite, we have\n",
        "\n",
        "$$\\begin{align*}\n",
        "g(z)&\\geq\\inf_x \\left(f_0(x)+\\sum_{i=1}^m\\lambda^*_i\\left(f_i(x)-z_i\\right)\\right) \\\\\n",
        "&=\\inf_x \\left(f_0(x)+\\sum_{i=1}^m\\lambda^*_i\\left(f_i(x)-y_i\\right)\\right) - \\sum_{i=1}^m\\lambda^*_i(z_i-y_i)\\\\\n",
        "& \\text{Strong duality} \\\\\n",
        "&=g(y) - \\sum_{i=1}^m\\lambda^*_i(z_i-y_i)\n",
        "\\end{align*}$$\n",
        "\n",
        "That is, $-\\lambda^*$ is a subgradient of $\\partial g(y)$"
      ],
      "metadata": {
        "id": "GTT3RKmZLCYm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DimhKwSjKe9e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}