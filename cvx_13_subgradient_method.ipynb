{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Subgradient method"
      ],
      "metadata": {
        "id": "xKboRmIyTbb8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subgradient method to minimize nondifferentiable convex function $f$\n",
        "\n",
        "$$x^{k+1}=x^k-\\alpha_k g^k, \\,\\, \\alpha_k>0$$\n",
        "\n",
        "* $g^k$ is `any` subgradient of $f$ at $x^k$\n",
        "* it is `not` a descent method, we keep track of best point\n",
        "$$f_{\\text{best}}^k=\\min_{i=1,\\cdots, k}f(x^i)$$"
      ],
      "metadata": {
        "id": "9YwMaD7WTgVL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step size are fixed ahead of time, rather than using some sort of line search as in gradient methods\n",
        "\n",
        "* $\\alpha_k$ can be constant\n",
        "* step length can be constant $\\alpha_k=\\gamma/\\|g^k\\|_2$, so $\\|x^{k+1}-x^k\\|_2=\\gamma$\n",
        "* `square summable but not summable`\n",
        "$$\\sum_{k=1}^{\\infty}\\alpha_k^2<\\infty,\\,\\sum_{k=1}^{\\infty}\\alpha_k=\\infty$$\n",
        "* `nonsummable diminishing` (e.g., $\\alpha_k=c/k$, etc.)\n",
        "$$\\lim_{k\\rightarrow \\infty}\\alpha_k=0,\\,\\sum_{k=1}^{\\infty}\\alpha_k=\\infty$$"
      ],
      "metadata": {
        "id": "s68OdjE1UJex"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Convergence analysis"
      ],
      "metadata": {
        "id": "SjXxAB7Qh3rR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To obtain bounds of\n",
        "\n",
        "$$f_{\\text{best}}^k-f(x^*)$$\n",
        "\n",
        "for various choices of step size, where $x^*$ solves unconstrained optimization problem $\\min f(x), f(x^*)=f^*$"
      ],
      "metadata": {
        "id": "pRSZ6txpi2Uq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assumption on $f$\n",
        "\n",
        "* convex, not necessarily smooth, not necessarily strongly convex\n",
        "* subgradient `uniformly bounded`\n",
        "$$\\forall x, \\forall g\\in \\partial f(x), \\|g\\|_2 \\leq G, G>0$$\n",
        "* $\\|x^1-x^*\\|_2\\leq R$"
      ],
      "metadata": {
        "id": "2zjseWDMkHIn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For subgradient steps\n",
        "\n",
        "$$x^{k+1}=x^k-\\alpha_k g^k, \\, g^k\\in \\partial f(x^k)$$\n",
        "\n",
        "we start by writing the gap between $x^k$ and $x^*$\n",
        "\n",
        "$$\\begin{align*}\n",
        "\\|x^{k+1}-x^*\\|_2^2 &= \\|x^k-\\alpha_k g^k-x^*\\|_2^2 \\\\\n",
        "& = \\|x^k-x^*\\|_2^2 -2\\alpha_k \\langle g^k, x^k-x^*\\rangle +\\alpha_k^2\\|g^k\\|_2^2 \\\\\n",
        "& \\left(f^*\\geq f(x^k) + \\langle g^k , x^*-x^k \\rangle \\Longrightarrow -\\langle g^k , x^k-x^* \\rangle \\leq f^*-f(x^k)\\right) \\\\\n",
        "& \\leq \\|x^k-x^*\\|_2^2 -2\\alpha_k \\left(f(x^k)-f^*\\right) +\\alpha_k^2\\|g^k\\|_2^2 \\\\\n",
        "& \\text{this shows when step size is small enough, we are making progress...} \\\\\n",
        "& \\text{apply the inequality recursively} \\\\\n",
        "&\\leq \\|x^1-x^*\\|_2^2-2\\sum_{i=1}^k\\alpha_i\\left(f(x^i)-f^*\\right)+\\sum_{i=1}^k\\alpha_i^2\\|g^i\\|_2^2 \\\\\n",
        "& \\text{rearrange using }\\|x^{k+1}-x^*\\|_2^2\\geq 0,\\, \\|x^1-x^*\\|_2\\leq R \\\\\n",
        "2\\sum_{i=1}^k\\alpha_i\\left(f(x^i)-f^*\\right) &\\leq R^2+\\sum_{i=1}^k\\alpha_i^2\\|g^i\\|_2^2 \\\\\n",
        "& \\text{since }\\sum_{i=1}^k\\alpha_i\\left(f(x^i)-f^*\\right)\\geq\\left(\\sum_{i=1}^k \\alpha_i\\right)(f_{\\text{best}}^k-f^*)\\\\\n",
        "f_{\\text{best}}^k-f^* &\\leq \\left(R^2+\\sum_{i=1}^k\\alpha_i^2\\|g^i\\|_2^2\\right)/\\left(2\\sum_{i=1}^k \\alpha_i\\right)\\\\\n",
        "& \\text{use }\\|g^k\\|_2 \\leq G \\\\\n",
        "&\\leq \\boxed{\\left(R^2+G^2\\sum_{i=1}^k\\alpha_i^2\\right)/\\left(2\\sum_{i=1}^k \\alpha_i\\right)}\n",
        "\\end{align*}$$"
      ],
      "metadata": {
        "id": "WA6M_ZTdh6b7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this, we have the following\n",
        "\n",
        "* constant step size $\\alpha_k=\\alpha$: converges to $G^2\\alpha/2$\n",
        "* constant step length $\\alpha_k=\\gamma/\\|g^k\\|_2$: converges to $G\\gamma /2$\n",
        "* square summable but not summable: converges to zero\n",
        "* nonsummable diminishing: converges to zero"
      ],
      "metadata": {
        "id": "8Ujyxg7zgh6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Bounded subgradient and Lipschitz continuous function"
      ],
      "metadata": {
        "id": "ZbzM22Jpj83_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\|g\\|_2\\leq G, G>0, \\forall x,\\forall g\\in \\partial f(x)$ implies $f$ is Lipschitz continuous with constant $G$\n",
        "\n",
        "$$|f(x)-f(y)|\\leq G\\|x-y\\|_2, \\forall x, y$$"
      ],
      "metadata": {
        "id": "K9aGSTCXkBQN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see this, let $g_x\\in \\partial f(x), g_y\\in \\partial f(y)$, we have\n",
        "\n",
        "$$g_x^T(x-y)\\geq f(x)-f(y)\\geq g_y^T(x-y)$$\n",
        "\n",
        "with Cauchy-Schwarz ($\\pm a^Tb\\leq \\|a\\|_2\\|b\\|_2$), we have\n",
        "\n",
        "$$G\\|x-y\\|_2\\geq f(x)-f(y)\\geq -G\\|x-y\\|_2$$"
      ],
      "metadata": {
        "id": "mslk-uMMkj9V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Optimal sequence"
      ],
      "metadata": {
        "id": "knZgz1cDiQtR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can choose sequence of positive $\\alpha_1, \\cdots, \\alpha_k$ such that\n",
        "\n",
        "$$\\left(R^2+G^2\\sum_{i=1}^k\\alpha_i^2\\right)/\\left(2\\sum_{i=1}^k \\alpha_i\\right)$$\n",
        "\n",
        "is minimized\n",
        "\n",
        "First, note that it is a convex function itself (quadratic over linear), and we can permute $\\alpha_i$ and the function value does not change\n",
        "\n",
        "Then, we apply all possible permutation to one optimal sequence $\\alpha^*$, and use Jensen's inequality\n",
        "\n",
        "$$f(\\mathbb{E}[\\alpha])\\leq \\mathbb{E}[f(\\alpha)]$$\n",
        "\n",
        "the right hand size would be just $f^*$ (since permutation does not change function value), while $\\mathbb{E}[\\alpha]$ (over all possible permutation) would be a sequence with identical $\\alpha_i$ and $f(\\mathbb{E}[\\alpha])$ must also be $f^*$\n",
        "\n",
        "Therefore, we can see that all $\\alpha_i$ must be equal in the optimal sequence"
      ],
      "metadata": {
        "id": "EaRkllcLiS4l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then write the bound as\n",
        "\n",
        "$$\\frac{R^2+G^2k\\alpha^2}{2k\\alpha}$$\n",
        "\n",
        "Take derivative and set it to zero, we get\n",
        "\n",
        "$$\\alpha=(R/G)/\\sqrt{k}$$\n",
        "\n",
        "Plug it back, we have\n",
        "\n",
        "$$f_{\\text{best}}^k-f^*\\leq RG/\\sqrt{k}$$"
      ],
      "metadata": {
        "id": "Y-rM8EAIvsa2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fi4EaRzIXafu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}