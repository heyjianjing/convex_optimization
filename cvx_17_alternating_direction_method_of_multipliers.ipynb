{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Recap"
      ],
      "metadata": {
        "id": "rVGBgv4lyhGE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For equality-constrained problem\n",
        "\n",
        "$$\\begin{align*}\n",
        "\\min\\,\\,\\, & f(x) \\\\\n",
        "\\text{s.t. }\\,\\,\\, & Ax= b \\\\\n",
        "\\end{align*}$$\n",
        "\n",
        "write out the Lagrangian\n",
        "\n",
        "$$L(x, \\nu)=f(x)+\\nu^T(Ax-b)$$\n",
        "\n",
        "we can do dual (sub)gradient method by starting with some initial $\\nu^0$ and repeating\n",
        "\n",
        "$$\\begin{align*}\n",
        "x^{k+1} &\\in \\arg \\min_x L(x, \\nu^k)\\\\\n",
        "\\nu^{k+1} &= \\nu^k+t_k(Ax^{k+1}-b)\n",
        "\\end{align*}$$\n",
        "\n",
        "The potential issue is that convergence requires stringent assumptions (such as strong convexity of $f$), and these iterates alone may not even ensure primal feasibility"
      ],
      "metadata": {
        "id": "L9KZPEoj0fyo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Augmented Lagrangian method"
      ],
      "metadata": {
        "id": "HNmi80jWe9lX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Augmented Lagrangian method (aka method of multipliers) modifies the problem, for $\\rho>0$\n",
        "\n",
        "$$\\begin{align*}\n",
        "\\min\\,\\,\\, & f(x)+\\frac{\\rho}{2}\\|Ax-b\\|_2^2 \\\\\n",
        "\\text{s.t. }\\,\\,\\, & Ax= b \\\\\n",
        "\\end{align*}$$\n",
        "\n",
        "Using a modified Lagrangian\n",
        "\n",
        "$$L_{\\rho}(x, \\nu)=f(x)+\\nu^T(Ax-b)+\\frac{\\rho}{2}\\|Ax-b\\|_2^2$$\n",
        "\n",
        "we get the iterates\n",
        "\n",
        "$$\\begin{align*}\n",
        "x^{k+1} &\\in \\arg \\min_x L_{\\rho}(x, \\nu^k)\\\\\n",
        "\\nu^{k+1} &= \\nu^k+\\rho(Ax^{k+1}-b)\n",
        "\\end{align*}$$"
      ],
      "metadata": {
        "id": "SkS1PRNve_ov"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice the step size choice $t_k=\\rho$\n",
        "\n",
        "Since $x^{k+1}$ minimizes $f(x)+\\left(\\nu^k\\right)^TAx+(\\rho/2)\\|Ax-b\\|_2^2$ over $x$, we have\n",
        "\n",
        "$$\\begin{align*}\n",
        "0&\\in \\partial f(x^{k+1})+A^T\\left(\\nu^k+\\rho(Ax^{k+1}-b)\\right) \\\\\n",
        "&=\\partial f(x^{k+1})+A^T\\nu^{k+1}\n",
        "\\end{align*}$$\n",
        "\n",
        "This is the `stationary condition` for the primal problem, so the KKT conditions are satisfied in the limit\n",
        "\n",
        "Therefore, it helps primal convergence under weaker assumptions\n",
        "\n",
        "However, we lose decomposability when $f$ is decomposable, since the Lagrangian is no longer trivially decomposable (due to the augmented term $\\rho/2\\|Ax-b\\|_2^2$)"
      ],
      "metadata": {
        "id": "7OWVi9aLf9d9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Alternating direction method of multipliers (ADMM)"
      ],
      "metadata": {
        "id": "2C_7QEfSUiCC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ADMM tries to `force` the decomposability on top of augmented Lagrangian method\n",
        "\n",
        "For the problem\n",
        "\n",
        "$$\\min_{x,z} f(x)+g(z), \\text{s.t. } Ax+Bz=c$$\n",
        "\n",
        "define augmented Lagrangian with $\\rho>0$\n",
        "\n",
        "$$L_{\\rho}(x, z, \\nu)=f(x)+g(z)+\\nu^T(Ax+Bz-c)+\\frac{\\rho}{2}\\|Ax+Bz-c\\|_2^2$$\n",
        "\n",
        "Instead of jointly optimizing primal variables $x, z$, ADMM repeats\n",
        "\n",
        "$$\\begin{align*}\n",
        "x^{k+1}&=\\arg \\min_x L_{\\rho}(x, z^k, \\nu^k) \\\\\n",
        "z^{k+1}&=\\arg \\min_z L_{\\rho}(x^{k+1}, z, \\nu^k) \\\\\n",
        "\\nu^{k+1}&=\\nu^k+\\rho(Ax^{k+1}+Bz^{k+1}-c) \\\\\n",
        "\\end{align*}$$\n",
        "\n",
        "Under modest assumptions on $f, g$ (e.g., closed and convex), ADMM iterates satisfy residual, primal and dual convergence"
      ],
      "metadata": {
        "id": "hdWsFA3rUlWb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Scaled form ADMM"
      ],
      "metadata": {
        "id": "uGmCa3bmaHl9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we denote $w=\\nu/\\rho$, we can rewrite the augmented Lagrangian as\n",
        "\n",
        "$$L_{\\rho}(x, z, w)=f(x)+g(z)+\\frac{\\rho}{2}\\|Ax+Bz-c+w\\|_2^2-\\frac{\\rho}{2}\\|w\\|_2^2$$\n",
        "\n",
        "and rewrite ADMM iterates as\n",
        "\n",
        "$$\\begin{align*}\n",
        "x^{k+1}&=\\arg \\min_x f(x)+\\frac{\\rho}{2}\\|Ax+Bz^{k}-c+w^k\\|_2^2 \\\\\n",
        "z^{k+1}&=\\arg \\min_z g(z)+\\frac{\\rho}{2}\\|Ax^{k+1}+Bz-c+w^k\\|_2^2 \\\\\n",
        "w^{k+1}&=w^k+Ax^{k+1}+Bz^{k+1}-c \\\\\n",
        "\\end{align*}$$\n",
        "\n",
        "where the kth iterate $w^k$ is a running sum of residuals\n",
        "\n",
        "$$w^k=w^0+\\sum_{i=1}^k(Ax^i+Bz^i-c)$$\n",
        "\n",
        "(We can see that $x^{k+1}$ and $z^{k+1}$ are basically `proximal` operators $\\text{prox}_{f, 1/\\rho}$ of some kind)"
      ],
      "metadata": {
        "id": "0QGwHc5EaKp9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XJ126mbTzM8q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}