{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### `Equality constrained` optimization"
      ],
      "metadata": {
        "id": "D4P76UVhbYme"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For twice-differentiable `convex` function $f: \\mathbf{R}^n \\rightarrow \\mathbf{R}$, we want to\n",
        "\n",
        "$$\\min f(x), \\text{s.t. } Ax=b$$\n",
        "\n",
        "where $A\\in \\mathbf{R}^{p \\times n}, \\, \\text{rank }A=p$ (`independent rows`), and $p^*$ is optimal value"
      ],
      "metadata": {
        "id": "19fx_-oNCIUd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can write out the if and only if `optimality condition`\n",
        "\n",
        "Start with Lagrangian\n",
        "\n",
        "$$L(x, \\nu)=f(x)+\\nu^T(Ax-b)$$\n",
        "\n",
        "With vanishing gradient of Lagrangian w.r.t. $x$, we have\n",
        "\n",
        "$$\\nabla_xL=\\nabla f(x)+A^T\\nu=0$$\n",
        "\n",
        "and together with primal feasibility, we have the optimality condition\n",
        "\n",
        "$$\\boxed{x\\in \\text{dom }f, \\, Ax=b, \\nabla f(x)+A^T\\nu = 0}$$\n",
        "\n",
        "for Lagrange multipliers $\\nu \\in \\mathbf{R}^p$"
      ],
      "metadata": {
        "id": "qw-HDXN1Ae_6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `Quadratic` example"
      ],
      "metadata": {
        "id": "-y5XzD5AKii1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\\min \\frac{1}{2}x^TPx+q^Tx+r, \\text{s.t. } Ax=b$$\n",
        "\n",
        "where $P\\in S^n_+, \\text{rank }A=p$"
      ],
      "metadata": {
        "id": "dgB5g1OxKltm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can write the optimality condition\n",
        "\n",
        "$$Ax=b,\\, Px+q+A^T\\nu=0$$\n",
        "\n",
        "which is a set of $n+p$ linear equations in $n+p$ variables\n",
        "\n",
        "$$\\begin{bmatrix}P & A^T \\\\\n",
        "A & 0\\end{bmatrix}\\begin{bmatrix}x \\\\ \\nu\\end{bmatrix}=\\begin{bmatrix}-q \\\\ b\\end{bmatrix}$$\n",
        "\n",
        "where the coefficient matrix is the `KKT matrix`"
      ],
      "metadata": {
        "id": "3k7yXS_xK5YB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The equivalent condition for `nonsingularity` of KKT matrix is\n",
        "\n",
        "* $\\text{rank}(\\begin{bmatrix}P \\\\ A\\end{bmatrix})=n$"
      ],
      "metadata": {
        "id": "nktQ8AIdLqks"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a) Apparently, KKT full rank $\\Longrightarrow$ first $n$ columns are independent"
      ],
      "metadata": {
        "id": "NG75xH0AN1th"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "b) To show that first $n$ columns in KKT are independent $\\Longrightarrow$ KKT invertible, we assume KKT is `not invertible`\n",
        "\n",
        "Then, there exists $x, \\nu$ that are `not both non-zero` such that\n",
        "\n",
        "$$Px+A^T\\nu=0, Ax=0$$\n",
        "\n",
        "or (left multiply by $x^T$ in first equation, and transpose and right multiply $\\nu$ in second equation)\n",
        "\n",
        "$$x^TPx+x^TA^T\\nu=0, x^TA^T\\nu=0$$\n",
        "\n",
        "Therefore, $x^TPx=0$\n",
        "\n",
        "Since $P$ is PSD, the only way this happens is $Px=0$\n",
        "\n",
        "To see this, as PSD matrices are diagonalizable, we can write\n",
        "\n",
        "$$x^TPx=(Q^Tx)^T\\Lambda (Q^Tx)=\\sum_{i}\\lambda_i(q_i^Tx)^2$$\n",
        "\n",
        "Because all $\\lambda_i\\geq 0$, therefore, the only way $x^TPx=0$ is $q_i^Tx=0$ for all $i$ corresponding to $\\lambda_i > 0$\n",
        "\n",
        "Therefore\n",
        "\n",
        "$$Px=Q\\Lambda Q^Tx = \\sum_{i}\\left[\\lambda_i(q_i^Tx)\\right]q_i=0$$"
      ],
      "metadata": {
        "id": "FE9qGSlLR23G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, we have\n",
        "\n",
        "$$\\begin{bmatrix}P\\\\A\\end{bmatrix}x=0$$\n",
        "\n",
        "Since $\\begin{bmatrix}P\\\\A\\end{bmatrix}$ has independent columns, the only way this happens is that $x=0$"
      ],
      "metadata": {
        "id": "GvajtHBzz9VM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With $Px=0$, we have must $A^T\\nu=0$\n",
        "\n",
        "Since $\\text{rank }A=p$, then, all of its $p$ rows (or $p$ columns of $A^T$) are independent, therefore $\\nu=0$  \n",
        "\n",
        "As a result, we have $x=\\nu=0$, which is a contradiction, thus, KKT is invertible"
      ],
      "metadata": {
        "id": "rwUZCNqG2G9H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Equality constrained `Newton step`"
      ],
      "metadata": {
        "id": "Hs3o2T1xG6BI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall Newton step in unconstrained optimization is based on 2nd order Taylor approximation of the function at some $x$\n",
        "\n",
        "$$\\begin{align*}\n",
        "\\Delta x_{nt}&=\\arg \\min_v f(x+v) \\\\\n",
        "&\\approx \\arg \\min_v f(x)+\\nabla f(x)^Tv+\\frac{1}{2}v^T\\nabla^2 f(x) v\n",
        "\\end{align*}$$\n",
        "\n",
        "With equality constraint (assume $x\\in \\text{dom }f$ and $Ax=b$)\n",
        "\n",
        "$$A(x+v)=b$$\n",
        "\n",
        "we can write the `optimality conditions` ($x\\in \\text{dom }f, \\, Ax=b, \\nabla f(x)+A^T\\nu = 0$) as (note, take derivative w.r.t. $v$, not $x$)\n",
        "\n",
        "$$\\boxed{A(x+v)=b ,\\, \\nabla f(x+v)+A^Tw \\approx \\nabla f(x) + \\nabla^2f(x)v+A^Tw=0}$$"
      ],
      "metadata": {
        "id": "2DQYNZ2QHE9Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using KKT matrix, and with $A(x+v)=b, Ax=b \\Longrightarrow Av=0$, we can see that the solution of $v$ that is `Newton step` solves\n",
        "\n",
        "$$\\begin{bmatrix}\n",
        "\\nabla^2 f(x) & A^T \\\\ A & 0\n",
        "\\end{bmatrix}\\begin{bmatrix}\n",
        "v\\\\w\n",
        "\\end{bmatrix}=\\begin{bmatrix}\n",
        "-\\nabla f(x)\\\\0\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "Since solution $v$ is in the `nullspace` of $A$, we know that applying Newton step would always keep $x+\\Delta x_{nt}$ `feasible`"
      ],
      "metadata": {
        "id": "uLS6tYeSIfId"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Equality constrained `Newton decrement`"
      ],
      "metadata": {
        "id": "eM-5ORYJaWyp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, we define Newton decrement as\n",
        "\n",
        "$$\\begin{align*}\n",
        "\\lambda(x)&= \\left(\\Delta x_{nt}^T \\nabla^2 f(x) \\Delta x_{nt}\\right)^{1/2} \\\\\n",
        "& = \\left(-\\nabla f(x)^T\\Delta x_{nt}\\right)^{1/2}\n",
        "\\end{align*}$$\n",
        "\n",
        "which is still the `Newton step measured using quadratic norm under the Hessian`"
      ],
      "metadata": {
        "id": "zs4ChreJJ8iZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `second expression` of $\\lambda(x)$ comes from the fact that Newton step satisfies KKT equations, so we have\n",
        "\n",
        "$$\\nabla^2 f(x)\\Delta x_{nt}+A^Tw=-\\nabla f(x)$$\n",
        "\n",
        "Multiply $\\Delta x_{nt}^T$ from the left on each side of first block equation\n",
        "\n",
        "$$\\Delta x_{nt}^T\\nabla^2 f(x)\\Delta x_{nt}+\\Delta x_{nt}^TA^Tw=-\\Delta x_{nt}^T\\nabla f(x)$$\n",
        "\n",
        "From second block equation we have the feasibility condition\n",
        "\n",
        "$$A\\Delta x_{nt}=0$$\n",
        "\n",
        "Plug into the first block equation\n",
        "\n",
        "$$\\Delta x_{nt}^T\\nabla^2 f(x)\\Delta x_{nt}=-\\Delta x_{nt}^T\\nabla f(x)$$\n",
        "\n",
        "Since right hand side is just a dot product, we have\n",
        "\n",
        "$$\\boxed{\\Delta x_{nt}^T\\nabla^2 f(x)\\Delta x_{nt}=-\\nabla f(x)^T \\Delta x_{nt}}$$"
      ],
      "metadata": {
        "id": "AQ3aprLFZPs7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`However`\n",
        "\n",
        "$$\\lambda(x)\\neq\\left(\\nabla f(x)^T \\left(\\nabla^2f(x)\\right)^{-1}\\nabla f(x)\\right)^{1/2}$$\n",
        "\n",
        "since in general constrained case\n",
        "\n",
        "$$\\Delta x_{nt}\\neq \\left(\\nabla^2f(x)\\right)^{-1}\\nabla f(x)$$"
      ],
      "metadata": {
        "id": "gc8SbIQ8KVFy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, we can relate Newton decrement to 2nd order approximation of $f$ at $x$ for equality-constrained case\n",
        "\n",
        "$$\\begin{align*}\n",
        "f(x)-\\inf_{u} \\hat{f}(x+u) &= f(x)-\\hat{f}(x+\\Delta x_{nt}) \\\\\n",
        "& = f(x) - \\left(f(x)+\\nabla f(x)^T\\Delta x_{nt}+\\frac{1}{2}\\Delta x_{nt}^T\\nabla^2f(x)\\Delta x_{nt}\\right) \\\\\n",
        "& \\Delta x_{nt}^T\\nabla^2 f(x)\\Delta x_{nt}=-\\nabla f(x)^T \\Delta x_{nt}\\\\\n",
        "& = \\Delta x_{nt}^T\\nabla^2 f(x)\\Delta x_{nt}-\\frac{1}{2}\\Delta x_{nt}^T\\nabla^2f(x)\\Delta x_{nt} \\\\\n",
        "&=\\boxed{\\frac{1}{2}\\lambda(x)^2}\n",
        "\\end{align*}$$\n",
        "\n",
        "We see that $\\frac{1}{2}\\lambda(x)^2$ again provides an estimate of $f(x)-p^*$ `based on 2nd order approximation` of $f$ at $x$"
      ],
      "metadata": {
        "id": "62I28ku6af3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `Newton's method` with equality constraints"
      ],
      "metadata": {
        "id": "b9BTSDW7LAyX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have steps for equality-constrained Newton's method\n",
        "\n",
        "Start with a `feasible point` $x\\in \\text{dom }f, Ax=b$\n",
        "* compute Newton `step` $\\Delta x_{nt}$ from KKT equation (i.e., solve for $v$) and Newton `decrement`\n",
        "$$\\lambda(x)=\\left(\\Delta x_{nt}^T \\nabla^2 f(x) \\Delta x_{nt}\\right)^{1/2}$$\n",
        "* stopping criterion\n",
        "$$\\frac{1}{2}\\lambda(x)^2 \\leq \\epsilon$$\n",
        "* line search for `step size`, starting at $t=1$, backtrack $t\\leftarrow \\beta t$ until\n",
        "$$\\begin{align*}f(x+t\\Delta x_{nt})&<f(x)+\\alpha t \\nabla f(x)^T\\Delta x_{nt} \\\\\n",
        "& = f(x)-\\alpha t \\lambda(x)^2\n",
        "\\end{align*}$$\n",
        "* update\n",
        "$$x\\leftarrow x+t\\Delta x_{nt}$$"
      ],
      "metadata": {
        "id": "0y6GIqbbLD_R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Newton step at `infeasible points`"
      ],
      "metadata": {
        "id": "Pv8v-VhaRWVx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we are currently at an infeasible point, meaning $Ax\\neq b$, then, we have $Av=-(Ax-b)$, and the KKT equation becomes\n",
        "\n",
        "$$\\begin{bmatrix}\n",
        "\\nabla^2 f(x) & A^T \\\\ A & 0\n",
        "\\end{bmatrix}\\begin{bmatrix}\n",
        "v\\\\w\n",
        "\\end{bmatrix}=-\\begin{bmatrix}\n",
        "\\nabla f(x)\\\\Ax-b\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "From the solution, we can see that if we take the `full` Newton step $\\Delta x_{nt}$, $x+\\Delta x_{nt}$ will be `feasible`, and the rest iterations will be taken care of based on the previous analysis on Netwon step at feasible points"
      ],
      "metadata": {
        "id": "vJX68LZZRcTV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Primal-dual interpretation"
      ],
      "metadata": {
        "id": "aIvfS-Jq7ckt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also more explicitly updates both primal and dual variables (previously, we didn't really touch the dual variable)\n",
        "\n",
        "If we write out `residual` from the optimality condition, we have\n",
        "\n",
        "$$r(x, \\nu)=\\begin{bmatrix}\\nabla f(x)+A^T\\nu \\\\ Ax-b \\end{bmatrix}$$\n",
        "\n",
        "Linearize and set it to zero (since we want residual to be zero), we have\n",
        "\n",
        "$$r(x, \\nu)+Dr(x, \\nu)\\begin{bmatrix} \\Delta x_{nt} \\\\ \\Delta v_{nt} \\end{bmatrix}=0$$\n",
        "\n",
        "For the Jacobian, we have\n",
        "\n",
        "$$Dr(x, \\nu)=\\begin{bmatrix}\\frac{\\partial r_1}{\\partial x} & \\frac{\\partial r_1}{\\partial \\nu}\\\\ \\frac{\\partial r_2}{\\partial x} & \\frac{\\partial r_2}{\\partial \\nu} \\end{bmatrix}=\\begin{bmatrix}\\nabla^2 f(x) & A^T\\\\ A & 0 \\end{bmatrix}$$\n",
        "\n",
        "Rearrange\n",
        "\n",
        "$$\\begin{bmatrix}\\nabla^2 f(x) & A^T\\\\ A & 0 \\end{bmatrix}\\begin{bmatrix} \\Delta x_{nt} \\\\ \\Delta v_{nt} \\end{bmatrix}=-\\begin{bmatrix}\\nabla f(x)+A^T\\nu \\\\ Ax-b \\end{bmatrix}$$\n",
        "\n",
        "where $w=\\nu + \\Delta v_{nt}$"
      ],
      "metadata": {
        "id": "h7Ln0g5B7ina"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Easy to check that if we consider $r(x,\\nu)$ as function of $x$ only, the above derivation gives\n",
        "\n",
        "$$\\begin{bmatrix}\n",
        "\\nabla^2 f(x) & A^T \\\\ A & 0\n",
        "\\end{bmatrix}\\begin{bmatrix}\n",
        "\\Delta x_{nt}\\\\ \\nu\n",
        "\\end{bmatrix}=-\\begin{bmatrix}\n",
        "\\nabla f(x)\\\\Ax-b\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "which is what we have previously"
      ],
      "metadata": {
        "id": "NO0h6ZkwKUXq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Newton's method with infeasible start"
      ],
      "metadata": {
        "id": "8K11rbLcW9Eb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start with a point $x\\in \\text{dom }f$ and $\\nu$\n",
        "* compute primal and dual Newton `step` $\\Delta x_{nt}, \\Delta \\nu_{nt}$\n",
        "* line search for `step size`, starting at $t=1$, backtrack $t\\leftarrow \\beta t$ until\n",
        "$$\\|r(x+t\\Delta x_{nt}, \\nu+t\\Delta \\nu_{nt})\\|_2\\leq (1-\\alpha t)\\|r(x, \\nu)\\|_2$$\n",
        "* update\n",
        "$$x\\leftarrow x+t\\Delta x_{nt}, \\nu\\leftarrow \\nu+t\\Delta \\nu_{nt}$$\n",
        "* terminate if $Ax=b$ and $\\|r(x,\\nu)\\|_2\\leq \\epsilon$\n",
        "\n",
        "The reason we cannot use function value for line search is that in order to get back to feasible set, function value may need to increase, so it is no longer a descent method"
      ],
      "metadata": {
        "id": "ELHDvkq1XE4C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Solving KKT equations"
      ],
      "metadata": {
        "id": "4aig1zXoAlVB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use LDLT factorization to solve KKT system of equations\n",
        "\n",
        "$$Kx=\\begin{bmatrix}H &A^T \\\\A & 0\\end{bmatrix}x=r$$\n",
        "\n",
        "We do the following\n",
        "\n",
        "* $K=LDL^T$\n",
        "* Forward substitution for $Ly=r$\n",
        "* Scaling for $Dz=y$\n",
        "* Back substitution for $L^Tx=z$"
      ],
      "metadata": {
        "id": "l8jlPu2xApT2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Toy example\n",
        "\n",
        "$$K = \\begin{bmatrix}2 & 1 & 1 \\\\ 1 & 2 & 0 \\\\ 1 &0 & 0 \\end{bmatrix}, r=-\\begin{bmatrix}1 \\\\ 2 \\\\ 3\\end{bmatrix}$$"
      ],
      "metadata": {
        "id": "e-OlSQTnA2DM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "np.set_printoptions(formatter={'float': '{: 0.4f}'.format})\n",
        "\n",
        "plt.style.use('dark_background')\n",
        "# color: https://matplotlib.org/stable/gallery/color/named_colors.htm"
      ],
      "metadata": {
        "id": "cIh-phD-BCSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ldlt_factorization(A):\n",
        "    # Assume A is symmetric\n",
        "    m = A.shape[0]\n",
        "    l_mat = A.copy().astype(float)\n",
        "    d_mat = np.zeros(m)\n",
        "\n",
        "    for k in range(m):\n",
        "        d_mat[k] = l_mat[k, k]\n",
        "        if l_mat[k, k] == 0:\n",
        "            raise ValueError('Matrix is singular')\n",
        "\n",
        "        l_mat[k+1:, k+1:] -= np.outer(l_mat[k+1:, k], l_mat[k+1:, k]) / l_mat[k, k]\n",
        "        l_mat[k:, k] /= l_mat[k, k]\n",
        "\n",
        "    return np.tril(l_mat), np.diag(d_mat)\n",
        "\n",
        "def forward_substitution(L, b):\n",
        "    m, n = L.shape\n",
        "    x = np.zeros(n)\n",
        "    for i in range(n):\n",
        "        x[i] = (b[i] - np.dot(L[i, :i], x[:i])) / L[i, i]\n",
        "    return x\n",
        "\n",
        "def back_substitution(R, b):\n",
        "    m, n = R.shape\n",
        "    x = np.zeros(n)\n",
        "    for i in range(n - 1, -1, -1):\n",
        "        x[i] = (b[i] - np.dot(R[i, i + 1:], x[i + 1:])) / R[i, i]\n",
        "    return x"
      ],
      "metadata": {
        "id": "XwGF2jT0JyBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k_mat = np.array([[2, 1, 1], [1, 2, 0], [1, 0, 0]])\n",
        "r = - np.array([1, 2, 3])\n",
        "\n",
        "try:\n",
        "    l_mat, d_mat = ldlt_factorization(k_mat)\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "\n",
        "# Forward\n",
        "y = forward_substitution(l_mat, r)\n",
        "\n",
        "# Scaling\n",
        "z = y / np.diag(d_mat)\n",
        "\n",
        "# Back\n",
        "x = back_substitution(l_mat.T, z)\n",
        "print(\"Solution:\\n\", x)\n",
        "\n",
        "# Compare to NumPy\n",
        "print(\"\\nNumPy solution:\\n\", np.linalg.solve(k_mat, r))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHW4cmArA6bB",
        "outputId": "e4cdfe35-e71f-491f-fbeb-c59c46844a75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solution:\n",
            " [-3.0000  0.5000  4.5000]\n",
            "\n",
            "NumPy solution:\n",
            " [-3.0000  0.5000  4.5000]\n"
          ]
        }
      ]
    }
  ]
}